getwd()
exit
version
exit
for(i in 1:length(naRepacedData[,1])) {
library(lattice)
sum.step <- with(totalSteps, tapply(totalSteps$steps, totalSteps$date, sum))
library(lattice
names(activityDataWithNAReplaced)[1] <- "day"
median(activityDataWithNAReplaced$steps, na.rm = TRUE)
## Loading Data
x <- 1:4
p <- x/sum(x)
temp <- rbind(x, p)
rownames(temp) <- c("X", "Prob")
temp
x <- 1:4
p <- x/sum(x)
temp <- rbind(x, p)
rownames(temp) <- c("X", "Prob")
temp
mean(temp)
mean(temp[x])
mean(temp[p])
mean(temp)
x <- 1:4
p <- x/sum(x)
temp <- rbind(x, p)
rownames(temp) <- c("X", "Prob")
temp
mean(temp)
1.0 * .1
1.0 * .1
2.0 * .2
3.0 * .3
4.0 * .4
clear
clr
1.0 * .1
2.0 * .2
3.0 * .3
4.0 * .4
y <- (.1,.4,.9,1.6)
y <- (0.1,0.4,0.9,1.6)
mean(0.1,0.4,0.9,1.6)
x <- 1:4
p <- x/sum(x)
temp <- rbind(x, p)
rownames(temp) <- c("X", "Prob")
temp
mean <- sum(temp[1, ]*temp[2, ])
mean
x <- c(0.18, -1.54, 0.42, 0.95)
w <- c(2, 1, 3, 1)
z <- x*w
mean(z)
x <- c(0.8, 0.47, 0.51, 0.73, 0.36, 0.58, 0.57, 0.85, 0.44, 0.42)
y <- c(1.39, 0.72, 1.55, 0.48, 1.19, -1.59, 1.23, -0.65, 1.49, 0.05)
fit.origin <- lm( y ~ x - 1 )
summary(fit.origin)
data(mtcars)
x <- c(0.8, 0.47, 0.51, 0.73, 0.36, 0.58, 0.57, 0.85, 0.44, 0.42)
y <- c(1.39, 0.72, 1.55, 0.48, 1.19, -1.59, 1.23, -0.65, 1.49, 0.05)
fit.origin <- lm( y ~ x - 1 )
summary(fit.origin)
data(mtcars)
fit <- lm(mpg ~ wt, mtcars)
summary(fit)
data(mtcars)
fit <- lm(mpg ~ wt, mtcars)
summary(fit)
data(mtcars)
fit <- lm(mpg ~ wt, mtcars)
summary(fit)
data(mtcars)
fit <- lm(mpg ~ wt, mtcars)
summary(fit)
x <- c(0.8, 0.47, 0.51, 0.73, 0.36, 0.58, 0.57, 0.85, 0.44, 0.42)
y <- c(1.39, 0.72, 1.55, 0.48, 1.19, -1.59, 1.23, -0.65, 1.49, 0.05)
fit.origin <- lm( y ~ x - 1 )
summary(fit.origin)
x <- c(0.18, -1.54, 0.42, 0.95)
w <- c(2, 1, 3, 1)
z <- x*w
mean(z)
x <- c(8.58, 10.46, 9.01, 9.64, 8.86)
m.x <- mean(x)
sd.x <- sd(x)
(x[1] - m.x)/sd.x
x <- c(0.8, 0.47, 0.51, 0.73, 0.36, 0.58, 0.57, 0.85, 0.44, 0.42)
y <- c(1.39, 0.72, 1.55, 0.48, 1.19, -1.59, 1.23, -0.65, 1.49, 0.05)
lm( y ~ x )
x <- c(0.8, 0.47, 0.51, 0.73, 0.36, 0.58, 0.57, 0.85, 0.44, 0.42)
mean(x)
x <- c(0.18, -1.54, 0.42, 0.95)
w <- c(2, 1, 3, 1)
optimize( function(u){ sum(w*(x-u)^2) }, interval=c(-100,100))
x <- c(0.61, 0.93, 0.83, 0.35, 0.54, 0.16, 0.91, 0.62, 0.62)
y <- c(0.67, 0.84, 0.6, 0.18, 0.85, 0.47, 1.1, 0.65, 0.36)
fit <- lm(y~x)
summary(fit)
e <- resid(fit)
sqe <- e*e
res.var <- sum(sqe) / (length(e) - 2)
sqrt(res.var)
data(mtcars)
attach(mtcars)
fit <- lm(mpg ~ wt, mtcars)
summary(fit)
exp <- fit$coefficients[1] + mean(wt) * fit$coefficients[2]
exp - 2 * 0.5591
?mtcars
summary(fit)
fit[[1]][1] + 3 * fit[[1]][2]
summary(fit)
2 * (fit$coefficients[2] - 2 * 0.5591)
attributes(fit)
w.c <- fit$residuals ^ 2
fit.c <- lm(mpg ~ 1, mtcars)
fit.c.res <- fit.c$residuals ^ 2
sum(fit.c.res)
sum(w.c) /sum(fit.c.res)
fit[[1]][1] + 3 * fit[[1]][2]
data(mtcars)
attach(mtcars)
fit <- lm(mpg ~ wt, mtcars)
fit[[1]][1] + 3 * fit[[1]][2]
summary(fit)
fit[[1]][1] + 3 * fit[[1]][2]
summary(fit)
fit[[1]][1] + 3 /fit[[1]][2]
data(mtcars)
fit <- lm(mpg ~ factor(cyl) + wt, mtcars)
summary(fit)
fit2 <- lm(mpg ~ factor(cyl), mtcars)
summary(fit2)
plot(fit2)
summary(fit3)
data(mtcars)
fit3 <- lm(mpg ~ factor(cyl)*wt, mtcars)
summary(fit3)
fit3
summary(fit3)
lm(mpg ~ I(wt * 0.5) + factor(cyl), data = mtcars)
x <- c(0.586, 0.166, -0.042, -0.614, 11.72)
y <- c(0.549, -0.026, -0.127, -0.751, 1.344)
fit <- lm(y ~ x)
lm.influence(fit)$hat
x <- c(0.586, 0.166, -0.042, -0.614, 11.72)
y <- c(0.549, -0.026, -0.127, -0.751, 1.344)
fit <- lm(y~x)
lm.influence(fit)$hat
dfbetas(fit)
x <- c(0.586, 0.166, -0.042, -0.614, 11.72)
y <- c(0.549, -0.026, -0.127, -0.751, 1.344)
# Give the hat diagonal for the most influential point
fit <- lm(y ~ x)
hatvalues(fit)
data(mtcars)
fit1 <- lm(mpg ~ factor(cyl) + wt, data = mtcars)
fit2 <- lm(mpg ~ factor(cyl) + wt + interaction(cyl, wt), data = mtcars)
# To compare model we usually use an anova table
# anova null hypothesis says that both models are the same.
compare <- anova(fit1, fit2)
compare$Pr
data(mtcars)
fit1 <- lm(mpg ~ factor(cyl) + wt, data = mtcars)
fit2 <- lm(mpg ~ factor(cyl) + wt + interaction(cyl, wt), data = mtcars)
# To compare model we usually use an anova table
# anova null hypothesis says that both models are the same.
compare <- anova(fit1, fit2)
compare$Pr
l1<-lm(mpg ~ factor(cyl)+wt, data = mtcars)
l2<-lm(mpg ~ factor(cyl)*wt, data = mtcars)
library(lmtest)
lrtest(l2,l1)
library(lmtest)
install.packages("lmtest")
l1<-lm(mpg ~ factor(cyl)+wt, data = mtcars)
l2<-lm(mpg ~ factor(cyl)*wt, data = mtcars)
library(lmtest)
lrtest(l2,l1)
data(mtcars)
fit1 <- lm(mpg ~ factor(cyl) + wt, data = mtcars)
fit2 <- lm(mpg ~ factor(cyl) + wt + interaction(cyl, wt), data = mtcars)
# To compare model we usually use an anova table
# anova null hypothesis says that both models are the same.
compare <- anova(fit1, fit2)
compare$Pr
Ho is rejected, because TS(t = `r th$statistic`) > qt(1-alpha,`r th$parameter`)( = `r qt(1-alpha,th$parameter)`).
install.packages("swirl")
library(swirl)
swirl()
plot(child ~ parent, galton)
exit
q
plot(jitter(child,4) ~ parent,galton)"
exit
q
q
adData = data.frame(diagnosis,predictors)
testIndex = createDataPartition(diagnosis, p = 0.50,list=FALSE)
training = adData[-testIndex,]
testing = adData[testIndex,]
library(AppliedPredictiveModeling)
library(caret)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
testIndex = createDataPartition(diagnosis, p = 0.50,list=FALSE)
training = adData[-testIndex,]
testing = adData[testIndex,]
library(AppliedPredictiveModeling)
library(caret)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
testIndex = createDataPartition(diagnosis, p = 0.50,list=FALSE)
training = adData[-testIndex,]
testing = adData[testIndex,]
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
IL_str <- grep("^IL", colnames(training), value = TRUE)
preProc <- preProcess(training[, IL_str], method = "pca", thresh = 0.9)
preProc$rotation
setwd("~/git/PracticalMachineLearning")
library(caret)
inTrain <- createDataPartition(y = segmentationOriginal$Case, list = FALSE)
train <- subset(segmentationOriginal, Case == "Train")
test <- subset(segmentationOriginal, Case == "Test")
data(segmentationOriginal)
library(AppliedPredictiveModeling)
library(AppliedPredictiveModeling)
library(AppliedPredictiveModeling)
data(segmentationOriginal)
library(caret)
inTrain <- createDataPartition(y = segmentationOriginal$Case, list = FALSE)
train <- subset(segmentationOriginal, Case == "Train")
test <- subset(segmentationOriginal, Case == "Test")
set.seed(125)
modFit <- train(Class ~ ., data = train, method = "rpart")
modFit$finalModel
modFit <- train(Class ~ ., data = train, method = "rpart")
modFit$finalModel
modFit <- train(Class ~ ., data = train, method = "rpart")
plot(modFit$finalModel, uniform = TRUE, main = "Classification Tree")
modFit <- train(Class ~ ., data = train, method = "rpart")
library(caret)
library(pgmm)
data(olive)
olive = olive[,-1]
library(randomForest)
#Fit a classification tree where Area is the outcome variable.
# Then predict the value of area for the following data frame using the tree command with all defaults
model <- train(Area ~ ., data = olive, method = "rpart2")
newdata = as.data.frame(t(colMeans(olive)))
predict(model, newdata = newdata)
library(ElemStatLearn)
install.packages("ElemStatLearn")
library(ElemStatLearn)
data(SAheart)
set.seed(8484)
train = sample(1:dim(SAheart)[1],size=dim(SAheart)[1]/2,replace=F)
trainSA = SAheart[train,]
testSA = SAheart[-train,]
set.seed(13234)
model <- train(chd ~ age + alcohol + obesity + tobacco + typea + ldl,
data = trainSA, method = "glm", family = "binomial")
missClass = function(values,prediction){sum(((prediction > 0.5)*1) != values)/length(values)}
missClass(testSA$chd, predict(model, newdata = testSA))
missClass(trainSA$chd, predict(model, newdata = trainSA))
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
set.seed(33833)
a <- randomForest(y ~ ., data = vowel.train, importance = FALSE)
b <- varImp(a)
library(caret)
a <- randomForest(y ~ ., data = vowel.train, importance = FALSE)
ls(package:randomForest)
install.packages("randomForest")
library(randomForest)
a <- randomForest(y ~ ., data = vowel.train, importance = FALSE)
b <- varImp(a)
data(vowel.train)
data(vowel.test)
set.seed(33833)
a <- randomForest(y ~ ., data = vowel.train, importance = FALSE)
b <- varImp(a)
order(b)
a <- randomForest(y ~ ., data = vowel.train, importance = true)
a <- randomForest(y ~ ., data = vowel.train, importance = T)
b <- varImp(a)
order(b)
a <- randomForest(y ~ ., data = vowel.train, importance = F)
b <- varImp(a)
a <- randomForest(y ~ ., data = vowel.train, importance = T)
b <- varImp(a)
order(b
order(b)
order(b)
data(vowel.train)
data(vowel.test)
set.seed(33833)
a <- randomForest(y ~ ., data = vowel.train, importance = T)
b <- varImp(a)
b
order(b)
a <- randomForest(y ~ ., data = vowel.train, importance = false)
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
set.seed(33833)
a <- randomForest(y ~ ., data = vowel.train, importance = false)
a <- randomForest(y ~ ., data = vowel.train, importance = FALSE)
b <- varImp(a)
a
b <- varImp(a)
a <- randomForest(y ~ ., data = vowel.train, importance = TRUE)
a
b <- varImp(a)
b
order(b)
