getwd()
exit
version
exit
for(i in 1:length(naRepacedData[,1])) {
library(lattice)
sum.step <- with(totalSteps, tapply(totalSteps$steps, totalSteps$date, sum))
library(lattice
names(activityDataWithNAReplaced)[1] <- "day"
median(activityDataWithNAReplaced$steps, na.rm = TRUE)
## Loading Data
x <- 1:4
p <- x/sum(x)
temp <- rbind(x, p)
rownames(temp) <- c("X", "Prob")
temp
x <- 1:4
p <- x/sum(x)
temp <- rbind(x, p)
rownames(temp) <- c("X", "Prob")
temp
mean(temp)
mean(temp[x])
mean(temp[p])
mean(temp)
x <- 1:4
p <- x/sum(x)
temp <- rbind(x, p)
rownames(temp) <- c("X", "Prob")
temp
mean(temp)
1.0 * .1
1.0 * .1
2.0 * .2
3.0 * .3
4.0 * .4
clear
clr
1.0 * .1
2.0 * .2
3.0 * .3
4.0 * .4
y <- (.1,.4,.9,1.6)
y <- (0.1,0.4,0.9,1.6)
mean(0.1,0.4,0.9,1.6)
x <- 1:4
p <- x/sum(x)
temp <- rbind(x, p)
rownames(temp) <- c("X", "Prob")
temp
mean <- sum(temp[1, ]*temp[2, ])
mean
x <- c(0.18, -1.54, 0.42, 0.95)
w <- c(2, 1, 3, 1)
z <- x*w
mean(z)
x <- c(0.8, 0.47, 0.51, 0.73, 0.36, 0.58, 0.57, 0.85, 0.44, 0.42)
y <- c(1.39, 0.72, 1.55, 0.48, 1.19, -1.59, 1.23, -0.65, 1.49, 0.05)
fit.origin <- lm( y ~ x - 1 )
summary(fit.origin)
data(mtcars)
x <- c(0.8, 0.47, 0.51, 0.73, 0.36, 0.58, 0.57, 0.85, 0.44, 0.42)
y <- c(1.39, 0.72, 1.55, 0.48, 1.19, -1.59, 1.23, -0.65, 1.49, 0.05)
fit.origin <- lm( y ~ x - 1 )
summary(fit.origin)
data(mtcars)
fit <- lm(mpg ~ wt, mtcars)
summary(fit)
data(mtcars)
fit <- lm(mpg ~ wt, mtcars)
summary(fit)
data(mtcars)
fit <- lm(mpg ~ wt, mtcars)
summary(fit)
data(mtcars)
fit <- lm(mpg ~ wt, mtcars)
summary(fit)
x <- c(0.8, 0.47, 0.51, 0.73, 0.36, 0.58, 0.57, 0.85, 0.44, 0.42)
y <- c(1.39, 0.72, 1.55, 0.48, 1.19, -1.59, 1.23, -0.65, 1.49, 0.05)
fit.origin <- lm( y ~ x - 1 )
summary(fit.origin)
x <- c(0.18, -1.54, 0.42, 0.95)
w <- c(2, 1, 3, 1)
z <- x*w
mean(z)
x <- c(8.58, 10.46, 9.01, 9.64, 8.86)
m.x <- mean(x)
sd.x <- sd(x)
(x[1] - m.x)/sd.x
x <- c(0.8, 0.47, 0.51, 0.73, 0.36, 0.58, 0.57, 0.85, 0.44, 0.42)
y <- c(1.39, 0.72, 1.55, 0.48, 1.19, -1.59, 1.23, -0.65, 1.49, 0.05)
lm( y ~ x )
x <- c(0.8, 0.47, 0.51, 0.73, 0.36, 0.58, 0.57, 0.85, 0.44, 0.42)
mean(x)
x <- c(0.18, -1.54, 0.42, 0.95)
w <- c(2, 1, 3, 1)
optimize( function(u){ sum(w*(x-u)^2) }, interval=c(-100,100))
x <- c(0.61, 0.93, 0.83, 0.35, 0.54, 0.16, 0.91, 0.62, 0.62)
y <- c(0.67, 0.84, 0.6, 0.18, 0.85, 0.47, 1.1, 0.65, 0.36)
fit <- lm(y~x)
summary(fit)
e <- resid(fit)
sqe <- e*e
res.var <- sum(sqe) / (length(e) - 2)
sqrt(res.var)
data(mtcars)
attach(mtcars)
fit <- lm(mpg ~ wt, mtcars)
summary(fit)
exp <- fit$coefficients[1] + mean(wt) * fit$coefficients[2]
exp - 2 * 0.5591
?mtcars
summary(fit)
fit[[1]][1] + 3 * fit[[1]][2]
summary(fit)
2 * (fit$coefficients[2] - 2 * 0.5591)
attributes(fit)
w.c <- fit$residuals ^ 2
fit.c <- lm(mpg ~ 1, mtcars)
fit.c.res <- fit.c$residuals ^ 2
sum(fit.c.res)
sum(w.c) /sum(fit.c.res)
fit[[1]][1] + 3 * fit[[1]][2]
data(mtcars)
attach(mtcars)
fit <- lm(mpg ~ wt, mtcars)
fit[[1]][1] + 3 * fit[[1]][2]
summary(fit)
fit[[1]][1] + 3 * fit[[1]][2]
summary(fit)
fit[[1]][1] + 3 /fit[[1]][2]
data(mtcars)
fit <- lm(mpg ~ factor(cyl) + wt, mtcars)
summary(fit)
fit2 <- lm(mpg ~ factor(cyl), mtcars)
summary(fit2)
plot(fit2)
summary(fit3)
data(mtcars)
fit3 <- lm(mpg ~ factor(cyl)*wt, mtcars)
summary(fit3)
fit3
summary(fit3)
lm(mpg ~ I(wt * 0.5) + factor(cyl), data = mtcars)
x <- c(0.586, 0.166, -0.042, -0.614, 11.72)
y <- c(0.549, -0.026, -0.127, -0.751, 1.344)
fit <- lm(y ~ x)
lm.influence(fit)$hat
x <- c(0.586, 0.166, -0.042, -0.614, 11.72)
y <- c(0.549, -0.026, -0.127, -0.751, 1.344)
fit <- lm(y~x)
lm.influence(fit)$hat
dfbetas(fit)
x <- c(0.586, 0.166, -0.042, -0.614, 11.72)
y <- c(0.549, -0.026, -0.127, -0.751, 1.344)
# Give the hat diagonal for the most influential point
fit <- lm(y ~ x)
hatvalues(fit)
data(mtcars)
fit1 <- lm(mpg ~ factor(cyl) + wt, data = mtcars)
fit2 <- lm(mpg ~ factor(cyl) + wt + interaction(cyl, wt), data = mtcars)
# To compare model we usually use an anova table
# anova null hypothesis says that both models are the same.
compare <- anova(fit1, fit2)
compare$Pr
data(mtcars)
fit1 <- lm(mpg ~ factor(cyl) + wt, data = mtcars)
fit2 <- lm(mpg ~ factor(cyl) + wt + interaction(cyl, wt), data = mtcars)
# To compare model we usually use an anova table
# anova null hypothesis says that both models are the same.
compare <- anova(fit1, fit2)
compare$Pr
l1<-lm(mpg ~ factor(cyl)+wt, data = mtcars)
l2<-lm(mpg ~ factor(cyl)*wt, data = mtcars)
library(lmtest)
lrtest(l2,l1)
library(lmtest)
install.packages("lmtest")
l1<-lm(mpg ~ factor(cyl)+wt, data = mtcars)
l2<-lm(mpg ~ factor(cyl)*wt, data = mtcars)
library(lmtest)
lrtest(l2,l1)
data(mtcars)
fit1 <- lm(mpg ~ factor(cyl) + wt, data = mtcars)
fit2 <- lm(mpg ~ factor(cyl) + wt + interaction(cyl, wt), data = mtcars)
# To compare model we usually use an anova table
# anova null hypothesis says that both models are the same.
compare <- anova(fit1, fit2)
compare$Pr
Ho is rejected, because TS(t = `r th$statistic`) > qt(1-alpha,`r th$parameter`)( = `r qt(1-alpha,th$parameter)`).
install.packages("swirl")
library(swirl)
swirl()
plot(child ~ parent, galton)
exit
q
plot(jitter(child,4) ~ parent,galton)"
exit
q
q
setwd("~/git/PracticalMachineLearning/quiz1")
library(AppliedPredictiveModeling)
library(caret)
data(AlzheimerDisease)
library(AppliedPredictiveModeling)
install.packages("AppliedPredictiveModeling")
install.packages("caret")
library(AppliedPredictiveModeling)
library(caret)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
testIndex = createDataPartition(diagnosis, p = 0.50,list=FALSE)
dim(adData)
training = adData[-testIndex,]
testing = adData[testIndex,]
dim(adData)
dim(training)
dim(testing)
data(concrete)
library(caret)
set.seed(975)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
summary(training)
head(training)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
trainIndex = createDataPartition(diagnosis, p = 0.50)
training = adData[trainIndex,]
testing = adData[-trainIndex,]
adData = data.frame(diagnosis,predictors)
testIndex = createDataPartition(diagnosis, p = 0.50,list=FALSE)
training = adData[-testIndex,]
testing = adData[testIndex,]
set.seed(975)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
featurePlot(x = training[,c('Cement',
'BlastFurnaceSlag',
'FlyAsh',
'Water',
'Superplasticizer',
'CoarseAggregate',
'FineAggregate', 'Age')],
y = training$CompressiveStrength )
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(975)
inTrain = createDataPartition(mixtures$CompressiveStrength,
p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
qplot(Superplasticizer, data=training)
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(975)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
featurePlot(x = training[,c('Cement',
'BlastFurnaceSlag',
'FlyAsh',
'Water',
'Superplasticizer',
'CoarseAggregate',
'FineAggregate', 'Age')],
y = training$CompressiveStrength )
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
ss <- training[,grep('^IL', x = names(training) )]
preProc <- preProcess(ss, method='pca', thresh = 0.8, outcome = training$diagnosis)
preProc
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
ss <- testing[,grep('^IL', x = names(testing) )]
model1 <- train(ss, testing$diagnosis, method='glm')
model2 <- preProcess(ss, method='pca', thresh = 0.8, outcome = testing$diagnosis)
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
ss <- testing[,grep('^IL', x = names(testing) )]
model1 <- train(ss, testing$diagnosis, method='glm')
library(AppliedPredictiveModeling)
library(caret)
data(AlzheimerDisease)
adData = data.frame(diagnosis, predictors)
trainIndex = createDataPartition(diagnosis, p = 0.5, list = FALSE)
training = adData[trainIndex, ]
testing = adData[-trainIndex, ]
dim(adData)
dim(training)
dim(testing)
